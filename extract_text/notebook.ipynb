{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5b9164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_jupyter_widescreen():\n",
    "    from IPython.display import display, HTML\n",
    "    \n",
    "    display(HTML(data=\"\"\"\n",
    "    <style>\n",
    "        div#notebook-container    {width: 95%; }\n",
    "        div#menubar-container     {width: 65%; }\n",
    "        div#maintoolbar-container {width: 99%; }\n",
    "    </style>\n",
    "    \"\"\"))\n",
    "set_jupyter_widescreen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae10479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing packages for text extraction\n",
    "# !/Volumes/develop/anaconda3/envs/llm/bin/pip install langchain\n",
    "# !/Volumes/develop/anaconda3/envs/llm/bin/pip install unstructured==0.5.6\n",
    "# !/Volumes/develop/anaconda3/envs/llm/bin/pip show pdfminer.six\n",
    "# !/Volumes/develop/anaconda3/envs/llm/bin/pip install --upgrade langchain pdfminer.six\n",
    "# !/Volumes/develop/anaconda3/envs/llm/bin/pip install pypdf\n",
    "# !/Volumes/develop/anaconda3/envs/llm/bin/pip install pymupdf\n",
    "# !/Volumes/develop/anaconda3/envs/llm/bin/pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4237a32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588da60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.document_loaders import UnstructuredFileLoader\n",
    "# loader = UnstructuredFileLoader('./sample_form_150522.pdf')\n",
    "# documents = loader.load()\n",
    "# documents_content = '\\n'.join(doc.page_content for doc in documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c0c820",
   "metadata": {},
   "source": [
    "#### It doesn't capture the ticked boxes in the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56ca543",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyMuPDFLoader(\"./sample_form_150522.pdf\")\n",
    "data = loader.load()\n",
    "#print(data[19].page_content)\n",
    "for i in range(82):\n",
    "    print(i, '---', data[i].page_content[-50:])\n",
    "doc_content = '\\n'.join(doc.page_content for doc in data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addd0c68",
   "metadata": {},
   "source": [
    "* split the documents into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaafae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(        \n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 200,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf097906",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc_content[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f799d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_chunks = text_splitter.split_text(doc_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ab10b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing some examples of the chunks and overlaps\n",
    "for i in range(len(doc_chunks)):\n",
    "    print(f'\\nchunk: {i}\\n')    \n",
    "    print(doc_chunks[i][:300])\n",
    "    print(''.join(50*['-']))\n",
    "    print(doc_chunks[i][-300:])\n",
    "    print(''.join(100*['*']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38fc268",
   "metadata": {},
   "source": [
    "* get openai embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee7024d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "found = load_dotenv(find_dotenv())\n",
    "if found:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "else:\n",
    "    print(\"couldn't find the key\")\n",
    "\n",
    "\n",
    "def get_doc_search(texts, embedding_model=None):\n",
    "    if embedding_model is None:\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "    else:\n",
    "        embeddings = OpenAIEmbeddings(model=embedding_model, deployment=embedding_model)\n",
    "    return FAISS.from_texts(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d53b2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b33d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_search = get_doc_search(doc_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8530dcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'how many employees do you have?'\n",
    "documents = doc_search.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d76f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in documents:\n",
    "    if 'employee' in d.page_content:\n",
    "        print(d.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b150e8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "llm = OpenAI(max_tokens=250,\n",
    "             temperature=0,\n",
    "             top_p=1,\n",
    "             frequency_penalty=0,\n",
    "             presence_penalty=0)\n",
    "chain = load_qa_chain(llm, chain_type = \"map_rerank\",  \n",
    "                      return_intermediate_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821ba5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import get_openai_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325da6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    results = chain({\"input_documents\":documents, \n",
    "                    \"question\": query},\n",
    "                    return_only_outputs=False)\n",
    "    print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "    print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "    print(f\"Total Cost (USD): ${cb.total_cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ba087b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = chain({\n",
    "                    \"input_documents\":documents, \n",
    "                    \"question\": query\n",
    "                }, \n",
    "                return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9decc73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e077b2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['intermediate_steps'][0]['answer'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0d85b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adopted from https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken\n",
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\"):\n",
    "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model in {\n",
    "        \"gpt-3.5-turbo-0613\",\n",
    "        \"gpt-3.5-turbo-16k-0613\",\n",
    "        \"gpt-4-0314\",\n",
    "        \"gpt-4-32k-0314\",\n",
    "        \"gpt-4-0613\",\n",
    "        \"gpt-4-32k-0613\",\n",
    "        }:\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif \"gpt-3.5-turbo\" in model:\n",
    "        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\")\n",
    "    elif \"gpt-4\" in model:\n",
    "        print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\"\n",
    "        )\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd80a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "model = 'text-davinci-003'\n",
    "model = 'gpt-3.5-turbo'\n",
    "encoding = tiktoken.encoding_for_model(model)\n",
    "#num_tokens_from_messages([doc.page_content for doc in documents], model=\"gpt-3.5-turbo-0613\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f964e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = 0\n",
    "for d in documents:\n",
    "    num_tokens += len(encoding.encode(d.page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feaf344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "found = load_dotenv(find_dotenv())\n",
    "if found:\n",
    "    openai.api_key  = os.getenv('OPENAI_API_KEY')\n",
    "else:\n",
    "    print(\"couldn't find the key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d47094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adopted from https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken\n",
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\"):\n",
    "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model in {\n",
    "        \"gpt-3.5-turbo-0613\",\n",
    "        \"gpt-3.5-turbo-16k-0613\",\n",
    "        \"gpt-4-0314\",\n",
    "        \"gpt-4-32k-0314\",\n",
    "        \"gpt-4-0613\",\n",
    "        \"gpt-4-32k-0613\",\n",
    "        }:\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif \"gpt-3.5-turbo\" in model:\n",
    "        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\")\n",
    "    elif \"gpt-4\" in model:\n",
    "        print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\"\n",
    "        )\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11d11bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.embeddings_utils import get_embedding\n",
    "\n",
    "# embedding model parameters\n",
    "embedding_model = \"text-embedding-ada-002\"\n",
    "embedding_encoding = \"cl100k_base\" \n",
    "max_tokens = 8000  # the maximum for text-embedding-ada-002 is 8191\n",
    "cost = 0.0001/1000 # dollar per tokens\n",
    "\n",
    "# get the token numbers for the document chunks\n",
    "encoding = tiktoken.encoding_for_model(model)\n",
    "print(f'encoding name for {embedding_model}: {encoding.name}')\n",
    "print(f\"{''.join(2*['-'])}\")\n",
    "#save embeddings\n",
    "df = pd.DataFrame()\n",
    "df['text'] = doc_chunks\n",
    "df['tokens'] = df['text'].apply(lambda x: len(encoding.encode(x)))\n",
    "df['embedding'] = df.text.apply(lambda x: get_embedding(x, engine=embedding_model))\n",
    "df['cost'] = cost*df['tokens']\n",
    "\n",
    "print(f\"maximum tokens:(chunk number:{df.sort_values(by='tokens', ascending=False).iloc[0].name}, token numbers: {df.sort_values(by='tokens', ascending=False).iloc[0].values[1]})\")\n",
    "\n",
    "print(f\"{''.join(2*['-'])}\")\n",
    "\n",
    "print(f'Total embedding cost: ${np.round(df.cost.sum(),4)}')\n",
    "df.to_csv(\"./sample_form_150522_chunks_with_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37ba390",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.embeddings_utils import  cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf54764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'how many employees do you have?'\n",
    "query_embedding = get_embedding(query, engine=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67db05b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:  could be a class with embedding attributes and other functions such as similarity search etc.\n",
    "def num_tokens(x, model='text-embedding-ada-002'):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(x))\n",
    "\n",
    "def calc_embedding_cost(x, cost=0.0001/1000):\n",
    "    return num_tokens(x)*cost\n",
    "\n",
    "\n",
    "def retrieve_relevant_text(query, vectordb, top_n=3, verbose=True):\n",
    "    # get the cost\n",
    "    cost = calc_embedding_cost(query)\n",
    "    \n",
    "    # get query's embedding\n",
    "    query_embedding = get_embedding(query, engine=embedding_model)\n",
    "\n",
    "    # search against the database and get the top matches\n",
    "    vectordb['similarity_score'] = vectordb.embedding.apply(lambda x: cosine_similarity(x, query_embedding))\n",
    "    result = vectordb.sort_values(by='similarity_score', ascending=False).head(top_n)\n",
    "    \n",
    "    # we sort the chunks  again according to their index in ascending order to maintain the semantic connectedness of the document\n",
    "    result = result.sort_index()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'query embedding cost:${cost}')\n",
    "    \n",
    "    return [(row[\"text\"], row['similarity_score']) for i, row in result.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14198370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_tempelate(query, vectordb, model, max_tokens = 1000, verbose=True):\n",
    "    \"\"\"Return a message for GPT, with relevant source texts pulled from a dataframe.\"\"\"\n",
    "    related_text = retrieve_relevant_text(query, vectordb, top_n=3, verbose=verbose)\n",
    "    intro = \"\"\"Answer the following Question based on the Context only. Only answer from the Context. If you don't know the answer, say 'I don't know'.\"\"\"\n",
    "    question = f\"\\n\\nQuestion: {query}\"\n",
    "    message = intro\n",
    "    for i in related_text:\n",
    "        next_text, _ = i\n",
    "        next_text = f'\\n\\n{next_text}\\n\\n'\n",
    "        if num_tokens(message + next_text + question, model=model) > max_tokens:\n",
    "            print('stopped using texts from the database...')\n",
    "            print(f'-- GPT prompt tokens exceeds maximum token limit ({max_tokens})')\n",
    "            break\n",
    "        else:\n",
    "            message += next_text\n",
    "    return message + question "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7d1076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(\n",
    "    query,\n",
    "    vectordb,\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    max_tokens=1000,\n",
    "    verbose=False,\n",
    "    temperature=0,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0\n",
    "):\n",
    "    \"\"\"Answers a query using GPT and a dataframe of relevant texts and embeddings.\"\"\"\n",
    "    message = prompt_tempelate(query, vectordb=vectordb, model=model, max_tokens=max_tokens, verbose=verbose)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "    ]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"{''.join(20*['-'])}\\n\")\n",
    "        print(message)\n",
    "        print(f\"{''.join(20*['-'])}\")\n",
    "        print(f'Total GPT cost: ${np.round(num_tokens_from_messages(messages=messages, model=model)*0.003/1000)}')\n",
    "        \n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=0,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "    response_message = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return response_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce1acb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask(query,df.copy(),verbose=False,max_tokens=1250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9174ac84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6703f9e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7927dfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = []\n",
    "test_list = [\n",
    "        (row[\"text\"], row['similarity_score'])\n",
    "        for i, row in tt.iterrows()\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77ec489",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579ad31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79c82c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get prompt messages\n",
    "def get_prompt(row):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Answer the following Question based on the Context only. Only answer from the Context. If you don't know the answer, say 'I don't know'.\n",
    "    Question: {row.question}\\n\\n\n",
    "    Context: {row.context}\\n\\n\n",
    "    Answer:\\n\"\"\",\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc8a7d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43cb845",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd269e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df.embedding.apply(lambda x: cosine_similarity(x, query_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f070bcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3352de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = df.merge(test, left_index=True, right_index=True).rename(columns={'embedding_y':'similarity_score'}).sort_values(by='similarity_score', ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d55530",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt[['text', 'similarity_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1708c496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6055f914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02682929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e43dfbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d1825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tt[['text', 'similarity_score']].values[0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
